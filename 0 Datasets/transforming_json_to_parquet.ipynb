{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming JSON to Parquet\n",
    "This notebook implements and tests the conversion from JSON to Parquet format to reduce the size of the dataset.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_JSON_to_parquet(json_file_path, parquet_dir, parquet_file_name, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Transforms the JSON file into a Parquet file in batches.\n",
    "\n",
    "    Parameters:\n",
    "    - json_file_path (str): Absolute path to the JSON file.\n",
    "    - parquet_dir (str): Directory to store the output Parquet file.\n",
    "    - parquet_file_name (str): Name of the output Parquet file.\n",
    "    - batch_size (int, optional): Number of lines to process in each batch. Default is 1000.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Open the JSON file\n",
    "        with open(json_file_path, 'r') as f:\n",
    "            batch_data = []\n",
    "            batch_count = 0\n",
    "\n",
    "            for line in f:\n",
    "                batch_data.append(json.loads(line))\n",
    "                if len(batch_data) == batch_size:\n",
    "                    # Process the current batch\n",
    "                    process_batch(batch_data, parquet_dir, parquet_file_name, batch_count)\n",
    "                    batch_data = []\n",
    "                    batch_count += 1\n",
    "\n",
    "            # Process any remaining data\n",
    "            if batch_data:\n",
    "                process_batch(batch_data, parquet_dir, parquet_file_name, batch_count)\n",
    "\n",
    "        print(f\"All batches processed successfully from {json_file_path}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {json_file_path} was not found.\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error: Failed to decode JSON. Details: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while loading the JSON file: {e}\")\n",
    "\n",
    "def process_batch(batch_data, parquet_dir, parquet_file_name, batch_count):\n",
    "    \"\"\"\n",
    "    Processes a batch of data and appends it to the Parquet file.\n",
    "\n",
    "    Parameters:\n",
    "    - batch_data (list): List of JSON objects in the current batch.\n",
    "    - parquet_dir (str): Directory to store the output Parquet file.\n",
    "    - parquet_file_name (str): Name of the output Parquet file.\n",
    "    - batch_count (int): The current batch number.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a DataFrame from the batch data\n",
    "        df = pd.DataFrame(batch_data)\n",
    "        print(f\"DataFrame for batch {batch_count} created successfully.\")\n",
    "\n",
    "        # Convert DataFrame to Arrow Table\n",
    "        table = pa.Table.from_pandas(df)\n",
    "        print(f\"Arrow Table for batch {batch_count} created successfully.\")\n",
    "\n",
    "        # Construct the file path for the Parquet file\n",
    "        parquet_file_path = os.path.join(parquet_dir, parquet_file_name)\n",
    "\n",
    "        # Check if the Parquet file already exists\n",
    "        if os.path.exists(parquet_file_path):\n",
    "            # Read the existing Parquet file\n",
    "            existing_table = pq.read_table(parquet_file_path)\n",
    "            # Combine the existing data with the new batch\n",
    "            combined_table = pa.concat_tables([existing_table, table])\n",
    "            # Write the combined data back to the Parquet file\n",
    "            pq.write_table(combined_table, parquet_file_path)\n",
    "        else:\n",
    "            # Write the new batch to the Parquet file\n",
    "            pq.write_table(table, parquet_file_path)\n",
    "        print(f\"Batch {batch_count} written successfully to {parquet_file_path}\")\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\"Error: Could not create DataFrame from batch data: {e}\")\n",
    "        raise\n",
    "    except pa.lib.ArrowInvalid as e:\n",
    "        print(f\"Error: Arrow Table creation failed for batch {batch_count}: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while writing batch {batch_count} to the Parquet file: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame for batch 0 created successfully.\n",
      "Arrow Table for batch 0 created successfully.\n",
      "Batch 0 written successfully to C:\\Users\\Fatema Kotb\\Documents\\CUFE 25\\Year 04\\Fall\\CMPS454 Natural Language Processing\\Fatema\\7. Group Project\\NLP-Project\\0 Datasets\\provided_datasets\\PIZZA_dev.parquet\n",
      "All batches processed successfully from C:\\Users\\Fatema Kotb\\Documents\\CUFE 25\\Year 04\\Fall\\CMPS454 Natural Language Processing\\Fatema\\7. Group Project\\NLP-Project\\0 Datasets\\provided_datasets\\PIZZA_dev.json.\n"
     ]
    }
   ],
   "source": [
    "base_dir = r\"C:\\Users\\Fatema Kotb\\Documents\\CUFE 25\\Year 04\\Fall\\CMPS454 Natural Language Processing\\Fatema\\7. Group Project\\NLP-Project\\0 Datasets\\provided_datasets\"\n",
    "\n",
    "json_file_path = os.path.join(base_dir, \"PIZZA_dev.json\")\n",
    "parquet_dir = base_dir\n",
    "parquet_file_name = \"PIZZA_dev.parquet\"\n",
    "\n",
    "transform_JSON_to_parquet(json_file_path, parquet_dir, parquet_file_name, batch_size=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
