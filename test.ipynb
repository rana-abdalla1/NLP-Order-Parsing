{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I have a parquet file with the following schema:\n",
    "        schema = pa.schema([\n",
    "            ('train_SRC', pa.string()),\n",
    "            ('cleaned_pizza_orders_src', pa.string()),\n",
    "            ('cleaned_drink_orders_src', pa.string())\n",
    "        ])\n",
    "train_SRC is an order\n",
    "cleaned_pizza_orders_src is a list of pizza suborders\n",
    "cleaned_drink_orders_src is a list of drink suborders\n",
    "\n",
    "I need to train a model to be able to extract the pizza and drink suborders from the order.\n",
    "\n",
    "Below is a preprocessing function\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "\n",
    "# Example of the input data schema (you would use your actual dataset)\n",
    "data = [\n",
    "    {\n",
    "        'train_SRC': \"one large pepperoni pizza and balsamic glaze pizza and a coke and 3 spirits\",\n",
    "        'cleaned_pizza_orders_src': \"one large pepperoni pizza, balsamic glaze pizza\",\n",
    "        'cleaned_drink_orders_src': \"a coke, 3 spirits\"\n",
    "    },\n",
    "    # Add more rows as needed...\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Preprocess the data to create tokenized input and labels\n",
    "def preprocess_data(df):\n",
    "    X_tokenized = []  # List of tokenized input sentences\n",
    "    y_labels = []     # Corresponding labels (pizza, drink, or neither)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        input_text = row['train_SRC']\n",
    "        pizza_text = row['cleaned_pizza_orders_src']\n",
    "        drink_text = row['cleaned_drink_orders_src']\n",
    "\n",
    "        # Tokenize the input sentence\n",
    "        tokens = input_text.lower().split()\n",
    "\n",
    "        # Create labels for each token: 0 for neither, 1 for pizza, 2 for drink\n",
    "        labels = []\n",
    "        pizza_tokens = pizza_text.lower().split()\n",
    "        drink_tokens = drink_text.lower().split()\n",
    "\n",
    "        pizza_in_order = False\n",
    "        drink_in_order = False\n",
    "        last_token_type = None  # None means no order detected yet\n",
    "\n",
    "        # Pizza: Start with 1, continue with 2, end with 3\n",
    "        # Drink: Start with 4, continue with 5, end with 6\n",
    "        # Neither: 0\n",
    "\n",
    "        for token in tokens:\n",
    "            if token in pizza_tokens:\n",
    "\n",
    "                if last_token_type == 'pizza':\n",
    "                    # Continue the pizza order\n",
    "                    labels.append(2)\n",
    "                else:\n",
    "                    # Is it a new pizza order? If so, end the previous drink order\n",
    "                    if last_token_type == 'drink':\n",
    "                        labels.append(6)\n",
    "                    # Start new pizza order\n",
    "                    labels.append(1)\n",
    "                    # Append the token\n",
    "                    labels.append(2)\n",
    "                    last_token_type = 'pizza'\n",
    "                \n",
    "            elif token in drink_tokens:\n",
    "                if last_token_type == 'drink':\n",
    "                    # Continue the drink order\n",
    "                    labels.append(5)\n",
    "                else:\n",
    "                    # Is it a new drink order? If so, end the previous pizza order\n",
    "                    if last_token_type == 'pizza':\n",
    "                        labels.append(3)\n",
    "                    # Start new drink order\n",
    "                    labels.append(4)\n",
    "                    # Append the token\n",
    "                    labels.append(5)\n",
    "                    last_token_type = 'drink'\n",
    "\n",
    "            else:\n",
    "                if last_token_type == 'pizza':\n",
    "                    labels.append(3)\n",
    "                elif last_token_type == 'drink':\n",
    "                    labels.append(6)\n",
    "                labels.append(0)\n",
    "                last_token_type = None             \n",
    "\n",
    "    return X_tokenized, y_labels\n",
    "\n",
    "# Apply preprocessing to the dataset\n",
    "X_tokenized, y_labels = preprocess_data(df)\n",
    "\n",
    "# Output the processed data with <START> and <END>\n",
    "for i in range(len(X_tokenized)):\n",
    "    print(f\"Input: {X_tokenized[i]}\")\n",
    "    print(f\"Labels: {y_labels[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "\n",
    "# Example of the input data schema (you would use your actual dataset)\n",
    "data = [\n",
    "    {\n",
    "        'train_SRC': \"one large pepperoni pizza and balsamic glaze pizza and a coke and 3 spirits\",\n",
    "        'cleaned_pizza_orders_src': \"one large pepperoni pizza, balsamic glaze pizza\",\n",
    "        'cleaned_drink_orders_src': \"a coke, 3 spirits\"\n",
    "    },\n",
    "    # Add more rows as needed...\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Preprocess the data to create tokenized input and labels\n",
    "def preprocess_data(df):\n",
    "    X_tokenized = []  # List of tokenized input sentences\n",
    "    y_labels = []     # Corresponding labels (pizza, drink, or neither)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        input_text = row['train_SRC']\n",
    "        pizza_text = row['cleaned_pizza_orders_src']\n",
    "        drink_text = row['cleaned_drink_orders_src']\n",
    "\n",
    "        # Tokenize the input sentence\n",
    "        tokens = input_text.lower().split()\n",
    "\n",
    "        # Create labels for each token: 0 for neither, 1 for pizza, 2 for drink\n",
    "        labels = []\n",
    "        pizza_tokens = pizza_text.lower().split()\n",
    "        drink_tokens = drink_text.lower().split()\n",
    "\n",
    "        pizza_in_order = False\n",
    "        drink_in_order = False\n",
    "        last_token_type = None  # None means no order detected yet\n",
    "\n",
    "        for token in tokens:\n",
    "            if token in pizza_tokens:\n",
    "                if last_token_type == 'pizza':\n",
    "                    labels.append(2)  # Continue the pizza order\n",
    "                else:\n",
    "                    if last_token_type == 'drink':  # End the previous drink order\n",
    "                        labels.append(6)\n",
    "                    labels.append(1)  # Start new pizza order\n",
    "                    labels.append(2)  # Inside pizza order\n",
    "                    last_token_type = 'pizza'\n",
    "\n",
    "            elif token in drink_tokens:\n",
    "                if last_token_type == 'drink':\n",
    "                    labels.append(5)  # Continue the drink order\n",
    "                else:\n",
    "                    if last_token_type == 'pizza':  # End the previous pizza order\n",
    "                        labels.append(3)\n",
    "                    labels.append(4)  # Start new drink order\n",
    "                    labels.append(5)  # Inside drink order\n",
    "                    last_token_type = 'drink'\n",
    "\n",
    "            else:\n",
    "                if last_token_type == 'pizza':\n",
    "                    labels.append(3)  # End the pizza order\n",
    "                elif last_token_type == 'drink':\n",
    "                    labels.append(6)  # End the drink order\n",
    "                labels.append(0)  # Neither order\n",
    "                last_token_type = None\n",
    "\n",
    "        X_tokenized.append(tokens)\n",
    "        y_labels.append(labels)\n",
    "\n",
    "    return X_tokenized, y_labels\n",
    "\n",
    "# Apply preprocessing to the dataset\n",
    "X_tokenized, y_labels = preprocess_data(df)\n",
    "\n",
    "# Output the processed data with <START> and <END>\n",
    "for i in range(len(X_tokenized)):\n",
    "    print(f\"Input: {X_tokenized[i]}\")\n",
    "    print(f\"Labels: {y_labels[i]}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
