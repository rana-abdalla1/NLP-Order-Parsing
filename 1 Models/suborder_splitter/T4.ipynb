{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5 Fine-Tuning for Sentence Splitting (Pizzas and Drinks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load and Prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema\n",
    "schema = pa.schema([\n",
    "    ('train_SRC', pa.string()),\n",
    "    ('cleaned_pizza_orders_src', pa.string()),\n",
    "    ('cleaned_drink_orders_src', pa.string())\n",
    "])\n",
    "\n",
    "# Function to format data for the model\n",
    "def format_data_for_model(input_data):\n",
    "    \"\"\"\n",
    "    Format the data for T5/BART model input.\n",
    "    \"\"\"\n",
    "    formatted_data = []\n",
    "    for row in input_data:\n",
    "        input_text = row['train_SRC']\n",
    "        output_text = f\"[PIZZAS] {row['cleaned_pizza_orders_src']} [DRINKS] {row['cleaned_drink_orders_src']}\"\n",
    "        formatted_data.append({'input': input_text, 'output': output_text})\n",
    "    return formatted_data\n",
    "\n",
    "# Load the preprocessed data from the Parquet file\n",
    "def load_and_format_data(file_path):\n",
    "    \"\"\"\n",
    "    Load data from a Parquet file and format it for the model.\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    print(f\"Loading data from {file_path}\")\n",
    "    table = pq.read_table(file_path)\n",
    "    df_splitter = table.to_pandas()\n",
    "    print(f\"Data loaded: {len(df_splitter)} records\")\n",
    "\n",
    "    # Print the headers of the input data\n",
    "    print(\"Headers of the input data:\", df_splitter.columns.tolist())\n",
    "\n",
    "    # Format the data for the model\n",
    "    print(\"Formatting data for the model\")\n",
    "    formatted_data = format_data_for_model(df_splitter.to_dict('records'))\n",
    "    print(f\"Data formatted: {len(formatted_data)} records\")\n",
    "\n",
    "    return formatted_data\n",
    "\n",
    "# Save the formatted data into a Parquet file\n",
    "def save_data_to_parquet(data, file_path):\n",
    "    \"\"\"\n",
    "    Save the formatted data into a Parquet file.\n",
    "    \"\"\"\n",
    "    print(f\"Saving data to {file_path}\")\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Define the schema for the formatted data\n",
    "    formatted_schema = pa.schema([\n",
    "        ('input', pa.string()),\n",
    "        ('output', pa.string())\n",
    "    ])\n",
    "    \n",
    "    table = pa.Table.from_pandas(df, schema=formatted_schema)\n",
    "    pq.write_table(table, file_path)\n",
    "    print(f\"Data saved: {len(data)} records\")\n",
    "\n",
    "# Load the saved data and tabulate it\n",
    "def load_and_tabulate_data(file_path):\n",
    "    \"\"\"\n",
    "    Load data from a Parquet file and tabulate it.\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    print(f\"Loading data from {file_path}\")\n",
    "    table = pq.read_table(file_path)\n",
    "    df = table.to_pandas()\n",
    "    print(f\"Data loaded: {len(df)} records\")\n",
    "\n",
    "    # Convert DataFrame to list of rows\n",
    "    rows = df.values.tolist()\n",
    "    headers = df.columns.tolist()\n",
    "\n",
    "    # Print data using tabulate\n",
    "    print(tabulate.tabulate(rows, headers=headers, tablefmt=\"grid\"))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for dataset loading - Replace this with actual loading logic\n",
    "# Dataset format: {'input': 'order text', 'output': 'split order'}\n",
    "dataset = load_dataset('csv', data_files={'train': 'train.csv', 'test': 'test.csv'})\n",
    "\n",
    "# Define a preprocessing function\n",
    "def preprocess_function(example):\n",
    "    return {\n",
    "        'input_text': f\"Split the order into pizzas and drinks: {example['input']}\",\n",
    "        'target_text': example['output']\n",
    "    }\n",
    "\n",
    "# Preprocess the dataset\n",
    "#! why remove_columns?\n",
    "#& After mapping preprocess_function, the dataset adds new fields (input_text, target_text). \n",
    "#& The original fields (input and output) are no longer needed and can cause redundancy or confusion. \n",
    "#& Removing them ensures that only the preprocessed fields are retained.\n",
    "dataset = dataset.map(preprocess_function, remove_columns=['input', 'output'])\n",
    "\n",
    "# Sample 500k if necessary (for 10-hour constraint)\n",
    "use_subset = True  # Set to False for full dataset\n",
    "if use_subset:\n",
    "    dataset['train'] = dataset['train'].shuffle(seed=42).select(range(500000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load the T5 Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"t5-small\"  # Use t5-small for faster fine-tuning\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Tokenize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    model_inputs = tokenizer(example['input_text'], max_length=128, truncation=True, padding='max_length')\n",
    "    labels = tokenizer(example['target_text'], max_length=128, truncation=True, padding='max_length')\n",
    "    #! why input_ids?\n",
    "    #& input_ids are the tokenized numerical representations of text. \n",
    "    #& In a sequence-to-sequence task like T5, the labels field specifies the expected output sequence during training. \n",
    "    #& The model uses labels to compute the loss and adjust weights during backpropagation.\n",
    "\n",
    "    #~ Why input_ids?\n",
    "    #~ Tokenization: The tokenizer converts input text into numerical IDs (input_ids), which represent the words or tokens in the input string.\n",
    "    #~ Labels for Supervised Training: The model needs the labels field to compute loss during training. \n",
    "    #~ By setting model_inputs['labels'] = labels['input_ids'], you provide the model with the expected output sequence for the corresponding input sequence.\n",
    "    #~ Why Necessary?: For sequence-to-sequence tasks like T5, the input (input_text) and expected output (target_text) must both be tokenized for the model to learn to map one to the other.\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize the datasets\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4  # Adjusted for 7GB GPU VRAM\n",
    "num_train_epochs = 15 if use_subset else 3\n",
    "\n",
    "gradient_accumulation_steps = 16  # Simulates larger effective batch size\n",
    "\n",
    "#! What is the purpose of this function?\n",
    "#& Purpose: This function defines a learning rate scheduler.\n",
    "#& Linear Scheduler: Gradually reduces the learning rate from the initial value to 0 as training progresses.\n",
    "#& Warmup Steps: Starts with a low learning rate and increases it linearly over the first 500 steps to stabilize training.\n",
    "#& Why Needed?: Improves training stability, especially with large models or noisy data.\n",
    "\n",
    "#~ How Are Both Done Together?\n",
    "#~ Linear Decay: The learning rate decreases linearly from the initial value to 0 over the course of training.\n",
    "#~ Warmup Steps: During the first 500 steps, the learning rate starts at 0 and increases linearly to the initial learning rate. This helps stabilize training by avoiding large updates at the start.\n",
    "#~ Combined Process: After the warmup period, the learning rate begins its linear decay. Together, this creates a two-phase schedule: warmup followed by decay.\n",
    "\n",
    "def get_scheduler():\n",
    "    from transformers import get_scheduler\n",
    "    return get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=None,  # Placeholder; will be attached during training\n",
    "        num_warmup_steps=500,\n",
    "        num_training_steps=(len(tokenized_datasets['train']) // batch_size) * num_train_epochs\n",
    "    )\n",
    "\n",
    "#! Explain these arguments weight_decay, save_total_limit, logging_steps, save_steps, warmup_steps\n",
    "#& weight_decay: Adds a small penalty to the weights to prevent overfitting by discouraging large weights during training.\n",
    "#& save_total_limit: Limits the number of saved checkpoints to save disk space. The two most recent checkpoints will be kept.\n",
    "#& logging_steps: Frequency of logging progress. Every 500 steps, training metrics (e.g., loss) are logged.\n",
    "#& save_steps: Frequency of saving model checkpoints. Every 1000 steps, the model checkpoint is saved.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,  # Simulates batch size of 64\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,  # Mixed precision for faster training\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=500,\n",
    "    save_steps=1000,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_steps=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Fine-Tuning with Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Save the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#! Same name?\n",
    "#& Saving both the model and tokenizer to the same directory (\"./t5_splitter_model\") ensures they can be loaded together for inference or further training. \n",
    "#& The tokenizer is essential for converting text inputs into numerical formats compatible with the model.\n",
    "model.save_pretrained(\"./t5_splitter_model\")\n",
    "tokenizer.save_pretrained(\"./t5_splitter_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_output(input_text):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=128, truncation=True, padding=True).to(device)\n",
    "    outputs = model.generate(**inputs, max_length=128, num_beams=5, early_stopping=True)\n",
    "    #! What is skip_special_tokens? What is the purpose of this function?\n",
    "    #& Purpose: Special tokens (e.g., <pad>, <eos>, <unk>) are used by the model for structural purposes but are not meaningful in the output text.\n",
    "    #& Example: The model might output a sequence like: <pad> This is the result <eos>.\n",
    "    #& By setting skip_special_tokens=True, only the meaningful part (This is the result) is returned, improving output readability.\n",
    "\n",
    "    #~ Does the Model Add These Tokens on Its Own?\n",
    "    #~ Yes: The model automatically appends special tokens to help with:\n",
    "    #~ Padding (<pad>): Ensures all sequences in a batch have the same length.\n",
    "    #~ End of Sequence (<eos>): Indicates the end of the generated sequence.\n",
    "    #~ Unknown Tokens (<unk>): Handles tokens not in the vocabulary.\n",
    "    #~ Purpose of skip_special_tokens=True: During decoding, this argument removes these structural tokens (e.g., <pad>, <eos>, <unk>) to produce a clean and readable output.\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test example\n",
    "input_text = \"Split the order into pizzas and drinks: I would like two large chicago pizzas and three cokes.\"\n",
    "output = generate_output(input_text)\n",
    "print(\"Generated Output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Notes\n",
    "- Adjust the dataset loading logic to your specific setup.\n",
    "- Use `use_subset=True` for faster training with the sampled dataset.\n",
    "- Use mixed precision (`fp16`) for faster and memory-efficient training.\n",
    "- Increase `gradient_accumulation_steps` if batch size needs to be simulated further.\n",
    "- Logs and results will be saved in `./logs` and `./results`."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
